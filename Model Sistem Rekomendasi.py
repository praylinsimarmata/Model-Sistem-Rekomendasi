# -*- coding: utf-8 -*-
"""Model Sistem Rekomendasi

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14YmpjlBpFqathl8AhTdzX1_GX1Rj3XEE

# **Sistem Rekomendasi Produk Amazon**

## **Import Library**
"""

import numpy as np
import pandas as pd
from datetime import datetime,timedelta

import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from matplotlib.ticker import MaxNLocator

import seaborn as sns
from IPython.display import Markdown, display
def printmd(string):
    display(Markdown(string))

"""## **Load Data**"""

!pip install -q Kaggle

from google.colab import files
uploaded = files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d saurav9786/amazon-product-reviews

! unzip /content/amazon-product-reviews.zip

"""## **Data Preparation**

### **Menampilkan data**
"""

# Menampilkan data yang terdapat dalam dataset
df=pd.read_csv("/content/ratings_Electronics (1).csv")
df.head()

"""### **Memberi Nama Kolom**"""

electronics_data=pd.read_csv("/content/ratings_Electronics (1).csv", dtype={'rating': 'int8'},
                             names=['userId', 'productId','rating','timestamp'], index_col=None, header=0)
electronics_data.head()

"""### **Melihat Info Data**"""

electronics_data.shape

electronics_data.describe()

electronics_data.info()

print("Jumlah Ratings: {:,}".format(electronics_data.shape[0]) )
print("Jenis Kolom Yang Tersedia: {}".format( np.array2string(electronics_data.columns.values)) )
print("Jumlah User: {:,}".format(len(electronics_data.userId.unique()) ) )
print("Jumlah Produk: {:,}".format(len(electronics_data.productId.unique())  ) )

"""### **Mengecek Baris Kosong**"""

print('Jumlah baris kosong:')
pd.DataFrame(electronics_data.isnull().sum().reset_index()).rename( columns={0:"Total Kosong","index":"Kolom"})

"""## **Exploratory Data Analysis**

### **Melihat data berdasarkan waktu dan ratingnya**
"""

data_by_date = electronics_data.copy()
data_by_date.timestamp = pd.to_datetime(electronics_data.timestamp, unit="s")
data_by_date = data_by_date.sort_values(by="timestamp", ascending=False).reset_index(drop=True)
print("Jumlah Ratings setiap harinya:")
data_by_date.groupby("timestamp")["rating"].count().tail(10).reset_index()

"""### **Melihat grafik trends berdasarkan waktu dan total pemberian rating**"""

data_by_date["year"]  = data_by_date.timestamp.dt.year
data_by_date["month"] = data_by_date.timestamp.dt.month
rating_by_year = data_by_date.groupby(["year","month"])["rating"].count().reset_index()
rating_by_year["date"] = pd.to_datetime(rating_by_year["year"].astype("str")  +"-"+rating_by_year["month"].astype("str") +"-1")
rating_by_year.plot(x="date", y="rating")
plt.title("Jumlah Rating setiap tahunnya")
plt.show()

"""### **Melihat banyaknya persebaran rating yang diberikan oleh pengguna**"""

ratings = electronics_data.groupby("rating").agg({"rating":"count"})

figsize = (15,5)
fig, (ax1,ax2) = plt.subplots(1,2,figsize=figsize)
ratings.plot.pie(ax=ax1,y="rating", legend=False, autopct='%1.f%%', startangle=90, fontsize="x-large", 
                 labels=["Bintang 1","Bintang 2","Bintang 3","Bintang 4","Bintang 5"])
ax1.set_ylabel('')
ratings.plot.bar(ax=ax2,  fontsize="large")
ax2.set(ylabel="Jumlah Rating")
ax2.set(title="Persebaran Data Rating")
ax2.get_legend().remove()

plt.show()

"""### **Melihat produk teratas berdasarkan jumlah rating**"""

rating_by_product = electronics_data.groupby("productId").agg({"userId":"count","rating":"mean"}).rename(
                        columns={"userId":"Jumlah Rating", "rating":"Rata-rata Rating"}).reset_index()

printmd("Produk teratas berdasarkan jumlah rating")
rating_by_product.sort_values(by="Jumlah Rating",ascending=False ).reset_index(drop=True).head()

"""## **Modelling and Result**

### **Import Library**
"""

!pip install -q tensorflow-recommenders

import numpy as np
import tensorflow as tf
import tensorflow_recommenders as tfrs

class RankingModel(tf.keras.Model):

    def __init__(self):
        super().__init__()
        embedding_dimension = 32

        self.user_embeddings = tf.keras.Sequential([
                                    tf.keras.layers.experimental.preprocessing.StringLookup(
                                        vocabulary=unique_userIds, mask_token=None),
                                    tf.keras.layers.Embedding(len(unique_userIds)+1, embedding_dimension)
                                    ])

        self.product_embeddings = tf.keras.Sequential([
                                    tf.keras.layers.experimental.preprocessing.StringLookup(
                                        vocabulary=unique_productIds, mask_token=None),
                                    tf.keras.layers.Embedding(len(unique_productIds)+1, embedding_dimension)
                                    ])
        self.ratings = tf.keras.Sequential([
                            tf.keras.layers.Dense(256, activation="relu"),
                            tf.keras.layers.Dense(64,  activation="relu"),
                            tf.keras.layers.Dense(1)
                              ])
    def call(self, userId, productId):
        user_embeddings  = self.user_embeddings (userId)
        product_embeddings = self.product_embeddings(productId)
        return self.ratings(tf.concat([user_embeddings,product_embeddings], axis=1))

# Build a model.
class amazonModel(tfrs.models.Model):

    def __init__(self):
        super().__init__()
        self.ranking_model: tf.keras.Model = RankingModel()
        self.task: tf.keras.layers.Layer   = tfrs.tasks.Ranking(
                                                    loss    =  tf.keras.losses.MeanSquaredError(),
                                                    metrics = [tf.keras.metrics.RootMeanSquaredError()])
            

    def compute_loss(self, features, training=False):
        rating_predictions = self.ranking_model(features["userId"], features["productId"]  )

        return self.task( labels=features["rating"], predictions=rating_predictions)

cutoff_no_rat = 50    
cutoff_year   = 2011  
recent_data   = data_by_date.loc[data_by_date["year"] > cutoff_year]
print("Jumlah Rating: {:,}".format(recent_data.shape[0]) )
print("Jumlah Pengguna: {:,}".format(len(recent_data.userId.unique()) ) )
print("Jumlah Produk: {:,}".format(len(recent_data.productId.unique())  ) )
del data_by_date  
recent_prod   = recent_data.loc[recent_data.groupby("productId")["rating"].transform('count').ge(cutoff_no_rat)].reset_index(
                    drop=True).drop(["timestamp","year","month"],axis=1)
del recent_data

userIds    = recent_prod.userId.unique()
productIds = recent_prod.productId.unique()
total_ratings= len(recent_prod.index)

ratings = tf.data.Dataset.from_tensor_slices( {"userId":tf.cast( recent_prod.userId.values  ,tf.string),
                                "productId":tf.cast( recent_prod.productId.values,tf.string),
                                "rating":tf.cast( recent_prod.rating.values  ,tf.int8,) } )

tf.random.set_seed(42)
shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)

train = shuffled.take( int(total_ratings*0.8) )
test = shuffled.skip(int(total_ratings*0.8)).take(int(total_ratings*0.2))

unique_productIds = productIds
unique_userIds    = userIds

"""### **Model Aktif Mempelajari Pola**"""

model = amazonModel()
model.compile(optimizer=tf.keras.optimizers.Adagrad( learning_rate=0.1 ), metrics=['accuracy'])
cached_train = train.shuffle(100_000).batch(8192).cache()
cached_test = test.batch(4096).cache()
model.fit(cached_train, epochs=20)

"""### **Result**"""

user_rand = userIds[123]
test_rating = {}
for m in test.take(5):
    test_rating[m["productId"].numpy()]=RankingModel()(tf.convert_to_tensor([user_rand]),
                                                       tf.convert_to_tensor([m["productId"]]))

print("5 produk teratas yang direkomendasikan untuk pengguna {}: ".format(user_rand))
for m in sorted(test_rating, key=test_rating.get, reverse=True):
    print(m.decode())

"""## **Evaluasi**"""

model.evaluate(cached_test, return_dict=True)